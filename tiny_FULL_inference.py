import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from PIL import Image
from torchvision import transforms
from transformers import CLIPTokenizer

# å¯¼å…¥æ‚¨æä¾›çš„æ¨¡å‹ç»“æ„ (å¿…é¡»å¯¼å…¥ï¼Œå› ä¸º torch.load éœ€è¦ç±»å®šä¹‰)
from tiny_student_model import LightweightStudentCLIP 
from image_encoder import CLIPSwiftFormerEncoder # ç¡®ä¿ image_encoder ä¹Ÿè¢«å¯¼å…¥

# --- è·¯å¾„å’Œé…ç½® ---
IMAGE_PATH = "/root/mnist-clip/data/RSICD_images/airport_1.jpg" # å›¾åƒè·¯å¾„

# æ³¨æ„ï¼šä½¿ç”¨æ‚¨ä¸Šä¸€è½®ä¿å­˜çš„ "é‡åŒ– Linear + Embedding å¹¶ä¿å­˜å®Œæ•´å¯¹è±¡" çš„æ–‡ä»¶è·¯å¾„
MODEL_PATH = "/root/mnist-clip/remoteclip_student_with_val2/quantized_FULL_OBJECT_INT8.pt" 

CANDIDATE_TEXTS = [ # æ–‡æœ¬æè¿°
    "some planes are parked in an airport",
    "A detailed illustration of a flying insect landing on a pink flower.",
    "A satellite image showing a lush green park with a river running through it.",
    "An aerial view of an urban area with tall skyscrapers and dense traffic.",
]
# CLIP æ ‡å‡† Tokenizer
TOKENIZER = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

# --- å›¾åƒé¢„å¤„ç† ---
image_transform = transforms.Compose([
    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=(0.48145466, 0.4578275, 0.40821073), 
        std=(0.26862954, 0.26130258, 0.27577711)
    )
])


def load_model():
    """
    åŠ è½½å®Œæ•´ä¿å­˜çš„é‡åŒ–æ¨¡å‹å¯¹è±¡ï¼Œå¼ºåˆ¶å…³é—­å®‰å…¨æ¨¡å¼ã€‚
    """
    print(f"ğŸš€ æ­¥éª¤ 1: å°è¯•åŠ è½½å®Œæ•´é‡åŒ–æ¨¡å‹å¯¹è±¡ (å¼ºåˆ¶ weights_only=False)...")
    try:
        # **å…³é”®ä¿®æ”¹ç‚¹ï¼šæ˜¾å¼è®¾ç½® weights_only=False**
        quantized_model = torch.load(
            MODEL_PATH, 
            map_location="cpu",
            weights_only=False  # ç¦ç”¨å®‰å…¨æ£€æŸ¥ï¼Œå…è®¸åŠ è½½è‡ªå®šä¹‰ç±»
        )
        quantized_model.eval()
        print("å®Œæ•´é‡åŒ–æ¨¡å‹å¯¹è±¡åŠ è½½æˆåŠŸï¼")
        return quantized_model.cpu()
    except Exception as e:
        print(f"\nâš ï¸ æœ€ç»ˆé”™è¯¯: æ— æ³•åŠ è½½æ¨¡å‹å¯¹è±¡ï¼Œè¯·æ£€æŸ¥ MODEL_PATHã€‚")
        print(f"åŸå§‹é”™è¯¯: {e}")
        return None


def run_inference(model: nn.Module, image_path: str, texts: list):
    """
    æ‰§è¡Œå›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦åŒ¹é…æ¨ç†ã€‚
    """
    if model is None:
        return
        
    # 1. å›¾åƒé¢„å¤„ç†
    try:
        image = Image.open(image_path).convert("RGB")
    except FileNotFoundError:
        print(f"\nâš ï¸ é”™è¯¯: æ‰¾ä¸åˆ°å›¾åƒæ–‡ä»¶: {image_path}")
        return
        
    image_input = image_transform(image).unsqueeze(0)

    # 2. æ–‡æœ¬é¢„å¤„ç†
    text_inputs = TOKENIZER(
        texts, 
        padding=True, 
        return_tensors="pt", 
        max_length=77, 
        truncation=True
    )

    # 3. ç‰¹å¾ç¼–ç ä¸ç›¸ä¼¼åº¦è®¡ç®—
    with torch.no_grad():
        try:
            # model.forward() è¿”å› logits_per_image, logits_per_text
            logits_per_image, _ = model(
                image=image_input,
                input_ids=text_inputs['input_ids'], 
                attention_mask=text_inputs['attention_mask']
            )
        except AttributeError as e:
            # æ•è·ä¹‹å‰è­¦å‘Šçš„é”™è¯¯
            print(f"\nâŒ æ¨ç†å¤±è´¥ï¼æ£€æµ‹åˆ° AttributeError: {e}")
            print("è¿™å¾ˆå¯èƒ½å°±æ˜¯å¯¹ HuggingFace æ–‡æœ¬ç¼–ç å™¨ä¸­çš„ nn.Embedding å±‚è¿›è¡Œé‡åŒ–å¯¼è‡´çš„å…¼å®¹æ€§é—®é¢˜ã€‚")
            print("è¯·ä½¿ç”¨åªé‡åŒ– Linear å±‚çš„æ¨¡å‹ (`quantized_LINEAR_ONLY_INT8.pt`) æ¥è¿›è¡Œæ¨ç†ã€‚")
            return

        # 4. Softmax è½¬æ¢ä¸ºç½®ä¿¡åº¦
        probs = F.softmax(logits_per_image, dim=-1)
        
    return probs.squeeze(0).tolist()


def display_results(probs: list, texts: list, image_name: str):
    """
    æ ¼å¼åŒ–è¾“å‡ºç»“æœï¼Œæ¨¡ä»¿é™„ä»¶ç…§ç‰‡æ•ˆæœã€‚
    """
    results = sorted(zip(probs, texts), key=lambda x: x[0], reverse=True)
    
    print("\n" + "="*70)
    print(f"å›¾ åƒ: {image_name}")
    print("-" * 70)
    
    best_match_text = results[0][1]
    best_match_prob = results[0][0] * 100
    
    for i, (prob, text) in enumerate(results, 1):
        prob_percent = prob * 100
        print(f"{i}. {prob_percent:.3f}% â†’ {text}")
        
    print("-" * 70)
    print(f"æœ€åŒ¹é…æè¿° (ç½®ä¿¡åº¦ {best_match_prob:.3f}%)")
    print(f"é¢„æµ‹ç»“æœ: \"{best_match_text}\"")
    print("="*70)


if __name__ == "__main__":
    # 1. åŠ è½½æ¨¡å‹
    quantized_model = load_model()
    
    # 2. æ‰§è¡Œæ¨ç†
    if quantized_model:
        probabilities = run_inference(quantized_model, IMAGE_PATH, CANDIDATE_TEXTS)
    
        # 3. å±•ç¤ºç»“æœ
        if probabilities:
            display_results(probabilities, CANDIDATE_TEXTS, os.path.basename(IMAGE_PATH))