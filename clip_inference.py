import torch
import torch.nn as nn
import torch.nn.functional as F
import os
from PIL import Image
from torchvision import transforms
from transformers import CLIPModel, CLIPProcessor
import numpy as np

# ----------------------------------------------------------------------
# æ•™å¸ˆæ¨¡å‹å°è£… (å¿…é¡»ä¸è®­ç»ƒæ—¶çš„ç»“æ„å®Œå…¨ä¸€è‡´)
# ----------------------------------------------------------------------

class CLIPTeacherModel(nn.Module):
    """
    å°è£… Hugging Face CLIP æ¨¡å‹ï¼Œç”¨äºåŠ è½½å¾®è°ƒåçš„æƒé‡ã€‚
    """
    def __init__(self, model_name: str = 'openai/clip-vit-base-patch32'):
        super().__init__()
        # 1. åŠ è½½å®Œæ•´çš„ CLIP æ¨¡å‹
        self.clip = CLIPModel.from_pretrained(model_name)
        self.logit_scale = self.clip.logit_scale

    # ä»…ä¿ç•™æ¨ç†æ‰€éœ€çš„ç‰¹å¾æå–æ–¹æ³•
    
    def get_image_features(self, images):
        """ ä»…è®¡ç®—å›¾åƒç‰¹å¾ (å½’ä¸€åŒ–åçš„åµŒå…¥) """
        vision_outputs = self.clip.vision_model(pixel_values=images)
        image_embeds = self.clip.visual_projection(vision_outputs.pooler_output)
        # CLIP ç‰¹å¾å¿…é¡»å½’ä¸€åŒ–
        image_features = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
        return image_features
    
    def get_text_features(self, input_ids, attention_mask=None):
        """ ä»…è®¡ç®—æ–‡æœ¬ç‰¹å¾ (å½’ä¸€åŒ–åçš„åµŒå…¥) """
        text_outputs = self.clip.text_model(input_ids=input_ids, attention_mask=attention_mask)
        text_embeds = self.clip.text_projection(text_outputs.pooler_output)
        # CLIP ç‰¹å¾å¿…é¡»å½’ä¸€åŒ–
        text_features = text_embeds / text_embeds.norm(dim=-1, keepdim=True)
        return text_features


# ----------------------------------------------------------------------
# æ¨ç†ä¸»å‡½æ•°
# ----------------------------------------------------------------------

@torch.no_grad()
def inference_clip(
    image_path: str,
    candidate_texts: list,
    model_weights_path: str,
    model_name: str = 'openai/clip-vit-base-patch32'
):
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # 1. åˆå§‹åŒ–æ¨¡å‹å’Œå¤„ç†å™¨
    teacher_model = CLIPTeacherModel(model_name=model_name).to(DEVICE)
    processor = CLIPProcessor.from_pretrained(model_name)
    
    # 2. åŠ è½½å¾®è°ƒåçš„æƒé‡
    try:
        # map_location ç¡®ä¿æƒé‡æ–‡ä»¶å¯ä»¥åœ¨ä»»ä½•è®¾å¤‡ä¸ŠåŠ è½½
        teacher_model.load_state_dict(torch.load(model_weights_path, map_location=DEVICE))
        print(f"âœ… æˆåŠŸåŠ è½½å¾®è°ƒæƒé‡: {model_weights_path}")
    except Exception as e:
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œæ¨¡å‹å°†ä½¿ç”¨é¢„è®­ç»ƒçš„ CLIP é»˜è®¤æƒé‡
        print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½æƒé‡æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è·¯å¾„æˆ–æ–‡ä»¶æ˜¯å¦å®Œæ•´ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        return None
    
    teacher_model.eval()
    
    # 3. å›¾åƒé¢„å¤„ç† (ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´)
    
    # ä¿®æ­£ CLIPProcessor å±æ€§è®¿é—®ï¼ˆå…¼å®¹æ–°ç‰ˆæœ¬ transformersï¼‰
    try:
        image_size = processor.image_processor.size['shortest_edge']
        image_mean = processor.image_processor.image_mean
        image_std = processor.image_processor.image_std
    except AttributeError:
        # å…¼å®¹æ—§ç‰ˆæœ¬ transformers
        print("è­¦å‘Š: ä½¿ç”¨æ—§ç‰ˆ processor å±æ€§è®¿é—®ã€‚")
        image_size = processor.size['shortest_edge']
        image_mean = processor.image_mean
        image_std = processor.image_std
    
    img_transform = transforms.Compose([
        transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(image_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=image_mean, std=image_std)
    ])
    
    try:
        img = Image.open(image_path).convert("RGB")
        image_tensor = img_transform(img).unsqueeze(0).to(DEVICE) # [1, 3, H, W]
    except FileNotFoundError:
        return f"é”™è¯¯: æ‰¾ä¸åˆ°å›¾åƒæ–‡ä»¶: {image_path}"
    except Exception as e:
        return f"é”™è¯¯: åŠ è½½æˆ–å¤„ç†å›¾åƒå¤±è´¥: {e}"

    # 4. æ–‡æœ¬ tokenization
    text_inputs = processor.tokenizer(
        candidate_texts, 
        padding='max_length', 
        truncation=True, 
        max_length=77, 
        return_tensors='pt'
    )
    input_ids = text_inputs['input_ids'].to(DEVICE)
    attention_mask = text_inputs['attention_mask'].to(DEVICE)

    # 5. ç‰¹å¾æå–
    image_features = teacher_model.get_image_features(image_tensor)
    text_features = teacher_model.get_text_features(input_ids, attention_mask)

    # 6. è®¡ç®—ç›¸ä¼¼åº¦ (ä½™å¼¦ç›¸ä¼¼åº¦)
    # ç›¸ä¼¼åº¦çŸ©é˜µ: [1, N_text]ï¼ŒèŒƒå›´åœ¨ [-1, 1]
    similarity_scores = (image_features @ text_features.T) 
    
    # ç»“æœè½¬ä¸º NumPy æ•°ç»„å¹¶æŒ¤å‹ç»´åº¦
    similarity_scores = similarity_scores.squeeze(0).cpu().numpy()

    # 7. æ‰¾å‡ºæœ€ä½³åŒ¹é…
    best_match_index = np.argmax(similarity_scores)
    best_match_text = candidate_texts[best_match_index]
    
    # 8. æ ¼å¼åŒ–è¾“å‡º
    results = {
        "image_path": image_path,
        "best_match_class": best_match_text,
        "similarity_results": {}
    }
    
    for i, text in enumerate(candidate_texts):
        # å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºç™¾åˆ†æ¯”å½¢å¼ï¼Œæˆ–ä»…ä¿ç•™å°æ•°ç‚¹åå››ä½
        similarity_value = float(similarity_scores[i])
        results["similarity_results"][text] = f"{similarity_value:.4f}"

    return results

# ----------------------------------------------------------------------
# è¿è¡Œç¤ºä¾‹
# ----------------------------------------------------------------------

if __name__ == "__main__":
    # --- é…ç½®å‚æ•° ---
    # æ ¹æ®æ‚¨çš„è¦æ±‚ï¼Œè®¾ç½®æƒé‡æ–‡ä»¶çš„è·¯å¾„
    PTH_PATH = '/root/mnist-clip/fine_tuned_clip_teacher.pth'
    
    # ğŸš¨ è¯·å°†æ­¤è·¯å¾„æ›¿æ¢ä¸ºæ‚¨è¦æ¨ç†çš„å®é™…å›¾åƒæ–‡ä»¶è·¯å¾„
    TEST_IMAGE = '/root/mnist-clip/RS_images_2800/RS_images_2800/dRiverLake/d012.jpg' 
    
    # é¥æ„Ÿå›¾åƒçš„å€™é€‰ç±»åˆ«æ–‡æœ¬æè¿°
    CANDIDATE_CLASSES = [
        "Grass",
        "Field",
        "Industry",
        "RiverLake",
        "Forest", # å¯¹åº” aForest
        "Resident",
        "Parking",
    ]
    
    # --- è¿è¡Œæ¨ç† ---
    print(f"æ­£åœ¨å¯¹å›¾åƒ {TEST_IMAGE} è¿›è¡Œæ¨ç†...")
    
    if not os.path.exists(TEST_IMAGE):
        print(f"\nâŒ æ‰¾ä¸åˆ°æµ‹è¯•å›¾åƒ {TEST_IMAGE}ã€‚è¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…å›¾åƒè·¯å¾„ã€‚")
    elif not os.path.exists(PTH_PATH):
        print(f"\nâŒ æ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶ {PTH_PATH}ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
    else:
        results = inference_clip(
            image_path=TEST_IMAGE,
            candidate_texts=CANDIDATE_CLASSES,
            model_weights_path=PTH_PATH
        )

        # --- æ‰“å°ç»“æœ ---
        if results is not None:
            print("\n--- æ¨ç†ç»“æœ ---")
            print(f"å›¾åƒè·¯å¾„: {results['image_path']}")
            print(f"é¢„æµ‹æœ€ä½³ç±»åˆ«: {results['best_match_class']}")
            print("\nä¸å„å€™é€‰æ–‡æœ¬çš„ç›¸ä¼¼åº¦ (ä½™å¼¦ç›¸ä¼¼åº¦, èŒƒå›´ -1.0000 åˆ° 1.0000):")
            
            # å¯¹ç›¸ä¼¼åº¦è¿›è¡Œæ’åºï¼Œä»¥ä¾¿æ›´æ¸…æ™°åœ°çœ‹åˆ°æœ€ä½³åŒ¹é…
            sorted_scores = sorted(
                results['similarity_results'].items(), 
                key=lambda item: float(item[1]), 
                reverse=True
            )
            
            for text, score in sorted_scores:
                print(f"  [ç›¸ä¼¼åº¦: {score}] - {text}")