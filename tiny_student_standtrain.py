import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
import json
import numpy as np
import time
import sys
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from transformers import CLIPTokenizer 

# --- ä¿®æ”¹ç‚¹ 1: å¯¼å…¥æ–°çš„è½»é‡åŒ–æ¨¡å‹ ---
try:
    # å‡è®¾ tiny_student_model.py å’Œ image_encoder.py åœ¨å½“å‰ç›®å½•ä¸‹
    from tiny_student_model import LightweightStudentCLIP
    from sklearn.model_selection import train_test_split
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿ tiny_student_model.py å’Œ image_encoder.py åœ¨å½“å‰ç›®å½•ä¸‹ï¼Œå¹¶ä¸”æ‚¨å·²å®‰è£…æ‰€æœ‰ä¾èµ–åº“ã€‚")
    sys.exit(1)


# --- 1. é…ç½®å‚æ•° ---
# æ•°æ®é›†è·¯å¾„
DATA_DIR = "/root/mnist-clip/data/RSICD_images" # å›¾åƒæ–‡ä»¶æ‰€åœ¨çš„æ ¹ç›®å½•
CAPTION_FILE_PATH = "/root/mnist-clip/data/RSICD-en_cleaned.json" 

# è®­ç»ƒå‚æ•°
BATCH_SIZE = 32
NUM_EPOCHS = 10
LEARNING_RATE = 5e-5
TRAIN_SPLIT_RATIO = 0.8
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Tokenizer ä½¿ç”¨çš„æ ‡å‡† CLIP æ¨¡å‹ (TinyCLIP çš„æ–‡æœ¬éƒ¨åˆ†é€šå¸¸å…¼å®¹æ ‡å‡† CLIP Tokenizer)
TOKENIZER_MODEL_NAME = 'openai/clip-vit-base-patch32'
# TinyCLIP é¢„è®­ç»ƒæƒé‡åç§° (ç”¨äºåˆå§‹åŒ– Student çš„æ–‡æœ¬éƒ¨åˆ†)
TINY_CLIP_MODEL_NAME = "wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M"

MAX_TEXT_LENGTH = 77 # CLIPæ ‡å‡†é•¿åº¦
WEIGHT_DECAY = 1e-4 


class RemoteSensingDataset(Dataset):
    def __init__(self, data_list, image_dir, tokenizer, transform):
        """
        Args:
            data_list (list): åŒ…å« {'imged_id': <filename>, 'caption': <caption>} çš„å­—å…¸åˆ—è¡¨ã€‚
            image_dir (str): å›¾åƒæ–‡ä»¶æ‰€åœ¨çš„æ ¹ç›®å½•ã€‚
            tokenizer: ç”¨äºæ–‡æœ¬ç¼–ç çš„ CLIPTokenizerã€‚
            transform: ç”¨äºå›¾åƒé¢„å¤„ç†çš„ torchvision.transformsã€‚
        """
        self.data_list = data_list
        self.image_dir = image_dir
        self.tokenizer = tokenizer
        self.transform = transform

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        item = self.data_list[idx]
        
        # 1. æå–æ–‡ä»¶åå¹¶æ›¿æ¢æ‰€æœ‰åæ–œæ  (\) ä¸ºæ­£æ–œæ  (/)
        image_filename_fixed = item['imged_id'].replace('\\', '/')
        
        # 2. æ„é€ æœ€ç»ˆè·¯å¾„
        img_path = os.path.join(self.image_dir, image_filename_fixed)
        
        # å›¾åƒå¤„ç†
        try:
            image = Image.open(img_path).convert("RGB")
            image = self.transform(image)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½å›¾åƒ {img_path}: {e}")
            return None 

        # æ–‡æœ¬å¤„ç†
        caption = item['caption']
        tokenized_text = self.tokenizer(
            caption, 
            padding='max_length', 
            truncation=True, 
            max_length=MAX_TEXT_LENGTH, 
            return_tensors="pt"
        )
        
        return image, tokenized_text['input_ids'].squeeze(), tokenized_text['attention_mask'].squeeze()

# --- 3. å›¾åƒé¢„å¤„ç†å’Œæ•°æ®åŠ è½½ ---
def load_and_split_data():
    if not os.path.exists(CAPTION_FILE_PATH):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æè¿°æ–‡ä»¶ï¼Œè¯·ç¡®ä¿æ–‡ä»¶ä½äº: {CAPTION_FILE_PATH}")
        sys.exit(1)
        
    with open(CAPTION_FILE_PATH, 'r', encoding='utf-8') as f:
        full_data = json.load(f)

    print(f"âœ… æˆåŠŸåŠ è½½ {len(full_data)} æ¡æ•°æ®ã€‚")

    # éšæœºåˆ’åˆ† 80% è®­ç»ƒé›†, 20% éªŒè¯é›†
    train_data, val_data = train_test_split(
        full_data, 
        test_size=(1 - TRAIN_SPLIT_RATIO), 
        random_state=42 
    )
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_data)} | éªŒè¯é›†å¤§å°: {len(val_data)}")
    
    # CLIP æ ‡å‡†å›¾åƒé¢„å¤„ç†
    preprocess = transforms.Compose([
        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], 
                              std=[0.26862954, 0.26130258, 0.27577711])
    ])
    
    # åˆå§‹åŒ– Tokenizer
    tokenizer = CLIPTokenizer.from_pretrained(TOKENIZER_MODEL_NAME)

    # å®ä¾‹åŒ– Dataset
    train_dataset = RemoteSensingDataset(train_data, DATA_DIR, tokenizer, preprocess) 
    val_dataset = RemoteSensingDataset(val_data, DATA_DIR, tokenizer, preprocess)
    
    def custom_collate_fn(batch):
        batch = [item for item in batch if item is not None]
        if not batch:
            return None, None, None
        
        images, input_ids, attention_masks = zip(*batch)
        
        return (
            torch.stack(images),
            torch.stack(input_ids),
            torch.stack(attention_masks)
        )

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)
    
    return train_loader, val_loader


# --- 4. è®­ç»ƒå’ŒéªŒè¯å‡½æ•° ---

def contrastive_loss(logits):
    """æ ‡å‡†çš„ CLIP å¯¹æ¯”å­¦ä¹ æŸå¤± (äº¤å‰ç†µ)"""
    targets = torch.arange(logits.shape[0]).long().to(DEVICE)
    return nn.CrossEntropyLoss()(logits, targets)

def train_one_epoch(model, dataloader, optimizer, scheduler, epoch): 
    model.train()
    total_loss = 0.0
    start_time = time.time()
    
    for step, batch in enumerate(dataloader):
        images, input_ids, attention_mask = batch
        
        if images is None:
            continue
            
        images = images.to(DEVICE)
        input_ids = input_ids.to(DEVICE)
        attention_mask = attention_mask.to(DEVICE)
        
        optimizer.zero_grad()
        
        # --- ä¿®æ”¹ç‚¹ 2: å‰å‘ä¼ æ’­è§£åŒ… ---
        # LightweightStudentCLIP åªè¿”å›ä¸¤ä¸ªå€¼ (logits_per_image, logits_per_text)
        logits_per_image, logits_per_text = model(
            images, 
            input_ids, 
            attention_mask
        )
        
        # è®¡ç®—æŸå¤±
        loss_i = contrastive_loss(logits_per_image)
        loss_t = contrastive_loss(logits_per_text)
        loss = (loss_i + loss_t) / 2
        
        loss.backward()
        optimizer.step()
        scheduler.step() 
        
        total_loss += loss.item()
        
        if (step + 1) % 50 == 0:
            avg_loss = total_loss / (step + 1)
            elapsed = time.time() - start_time
            print(f"Epoch {epoch}/{NUM_EPOCHS} | Step {step+1} | Loss: {avg_loss:.4f} | Time: {elapsed:.2f}s")
    
    return total_loss / len(dataloader)


@torch.no_grad()
def validate(model, dataloader):
    model.eval()
    total_val_loss = 0.0
    
    for batch in dataloader:
        images, input_ids, attention_mask = batch
        
        if images is None:
            continue
            
        images = images.to(DEVICE)
        input_ids = input_ids.to(DEVICE)
        attention_mask = attention_mask.to(DEVICE)
        
        # --- ä¿®æ”¹ç‚¹ 3: éªŒè¯é›†å‰å‘ä¼ æ’­è§£åŒ… ---
        logits_per_image, logits_per_text = model(
            images, 
            input_ids, 
            attention_mask
        )
        
        loss_i = contrastive_loss(logits_per_image)
        loss_t = contrastive_loss(logits_per_text)
        loss = (loss_i + loss_t) / 2
        
        total_val_loss += loss.item()

    return total_val_loss / len(dataloader)


# --- 5. ä¸»ç¨‹åº ---
def main():
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ LightweightStudentCLIP æ¨¡å‹ (è®¾å¤‡: {DEVICE})")
    print(f"ğŸ“š æ•°æ®ç›®å½•: {DATA_DIR} | æè¿°æ–‡ä»¶: {CAPTION_FILE_PATH}")

    # æ­¥éª¤ 1: åŠ è½½æ•°æ®å¹¶åˆ’åˆ†
    train_loader, val_loader = load_and_split_data()

    # æ­¥éª¤ 2: åˆå§‹åŒ–æ¨¡å‹
    # --- ä¿®æ”¹ç‚¹ 4: å®ä¾‹åŒ– LightweightStudentCLIP ---
    print("Initializing Model...")
    model = LightweightStudentCLIP(
        vision_variant='L1', # å¯é€‰ L1, L2, L3, L4 (ç¡®ä¿ä¸ image_encoder.py æ”¯æŒçš„ä¸€è‡´)
        projection_dim=512,
        tinyclip_model_name=TINY_CLIP_MODEL_NAME
    ).to(DEVICE)
    
    # ************************************************
    # *** å†»ç»“æ–‡æœ¬ç¼–ç å™¨å‚æ•° ***
    # ************************************************
    print("Freezing Text Encoder parameters...")
    # LightweightStudentCLIP åŒæ ·ä½¿ç”¨äº† self.text_model å’Œ self.text_projection
    try:
        for param in model.text_model.parameters():
            param.requires_grad = False
        print("âœ… æ–‡æœ¬ Transformer å‚æ•°å·²å†»ç»“ã€‚")
    except AttributeError:
        print("âš ï¸ è­¦å‘Š: æ— æ³•æ‰¾åˆ° model.text_modelã€‚")
    
    try:
        for param in model.text_projection.parameters():
            param.requires_grad = False
        print("âœ… æ–‡æœ¬æŠ•å½±å±‚å‚æ•°å·²å†»ç»“ã€‚")
    except AttributeError:
        print("âš ï¸ è­¦å‘Š: æ— æ³•æ‰¾åˆ° model.text_projectionã€‚")
    
    # ç¡®ä¿ logit_scale (æ¸©åº¦ç³»æ•°) å¯è®­ç»ƒ
    try:
        model.logit_scale.requires_grad = True
    except AttributeError:
        pass

    # ************************************************
    # *** åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ ***
    # ************************************************
    
    no_decay = ["bias", "LayerNorm.weight"]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() 
                       if not any(nd in n for nd in no_decay) and p.requires_grad],
            "weight_decay": WEIGHT_DECAY,
        },
        {
            "params": [p for n, p in model.named_parameters() 
                       if any(nd in n for nd in no_decay) and p.requires_grad],
            "weight_decay": 0.0,
        },
    ]

    optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE) 
    
    total_steps = len(train_loader) * NUM_EPOCHS
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(total_steps * 0.1),
        num_training_steps=total_steps
    )
    
    best_val_loss = float('inf')
    
    # æ­¥éª¤ 3: è®­ç»ƒå¾ªç¯
    for epoch in range(1, NUM_EPOCHS + 1):
        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, epoch) 
        val_loss = validate(model, val_loader)
        
        print(f"\n======== Epoch {epoch} Summary ========")
        print(f"Average Training Loss: {train_loss:.4f}")
        print(f"Average Validation Loss: {val_loss:.4f}")
        print("=======================================\n")
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model_save_path = f"/root/mnist-clip/tiny_student_finetuned.pt"
            torch.save(model.state_dict(), model_save_path)
            print(f"âœ¨ éªŒè¯æŸå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ {model_save_path}")

if __name__ == "__main__":
    main()