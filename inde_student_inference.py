import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import CLIPTokenizer, CLIPModel, CLIPProcessor
from torchvision import transforms
from PIL import Image
import os
import sys

# å¿…é¡»å¯¼å…¥æ‚¨çš„ StudentCLIP ç±»ï¼Œå®ƒä¾èµ–äº image_encoder.py
# ç¡®ä¿ StudentCLIP.py å’Œ image_encoder.py æ–‡ä»¶å­˜åœ¨äºå½“å‰ç›®å½•
try:
    from StudentCLIP import StudentCLIP
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿ StudentCLIP.py æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹ï¼Œå¹¶ä¸”å®ƒèƒ½æ­£ç¡®å¯¼å…¥ image_encoderã€‚")
    sys.exit(1)


# ----------------------------------------------------------------------
# 1. æ¨ç†ä¸»å‡½æ•°
# ----------------------------------------------------------------------
@torch.no_grad()
def inference():
    # === é…ç½®åŒºåŸŸ (ä¸æ‚¨çš„è¦æ±‚å’Œå…ˆå‰è„šæœ¬ä¿æŒä¸€è‡´) ===
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    TEACHER_MODEL_NAME = 'openai/clip-vit-base-patch32'
    
    # æƒé‡è·¯å¾„
    MODEL_WEIGHTS_PATH = '/root/mnist-clip/student_clip_best_model.pt' 
    
    # é›¶æ ·æœ¬åˆ†ç±»è¾“å…¥
    IMAGE_PATH = '/root/mnist-clip/RS_images_2800/RS_images_2800/fResident/f003.jpg'
    LABELS = [
        "grass", "field", "Industry", "riverlake", 
        "forest", "resident", "parking"
    ]
    # ä½¿ç”¨ä¸Šä¸‹æ–‡ Prompt æé«˜å‡†ç¡®æ€§
    TEXT_PROMPTS = [f"a remote sensing image of {label}." for label in LABELS]
    
    print("-" * 60)
    print("ğŸš€ æ­£åœ¨åŠ è½½è®­ç»ƒå®Œæˆçš„å­¦ç”Ÿæ¨¡å‹...")
    print(f"ğŸ”„ å½“å‰è®¾å¤‡: {DEVICE}")

    # === 1. åˆå§‹åŒ–æ¨¡å‹ä¸å¤„ç†å™¨ ===
    model = StudentCLIP(teacher_model_name=TEACHER_MODEL_NAME).to(DEVICE)
    processor = CLIPProcessor.from_pretrained(TEACHER_MODEL_NAME)
    
    # === 2. åŠ è½½æƒé‡ ===
    if os.path.exists(MODEL_WEIGHTS_PATH):
        try:
            model.load_state_dict(torch.load(MODEL_WEIGHTS_PATH, map_location=DEVICE))
            print(f"âœ… æˆåŠŸåŠ è½½å­¦ç”Ÿæ¨¡å‹æƒé‡: {MODEL_WEIGHTS_PATH}")
        except RuntimeError as e:
            print(f"âŒ é”™è¯¯: æƒé‡æ–‡ä»¶åŠ è½½å¤±è´¥ (ç»“æ„å¯èƒ½ä¸åŒ¹é…): {e}")
            return
    else:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ {MODEL_WEIGHTS_PATH}ã€‚è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return
        
    model.eval()

    # === 3. æ•°æ®é¢„å¤„ç† ===
    try:
        image = Image.open(IMAGE_PATH).convert("RGB")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°å›¾åƒæ–‡ä»¶ {IMAGE_PATH}")
        return
    
    # å›¾åƒé¢„å¤„ç†
    image_inputs = processor.image_processor(image, return_tensors="pt")
    image_tensor = image_inputs.pixel_values.to(DEVICE)
    
    # æ–‡æœ¬å¤„ç†
    text_inputs = processor.tokenizer(
        TEXT_PROMPTS, 
        padding=True, 
        truncation=True, 
        return_tensors="pt"
    )
    input_ids = text_inputs.input_ids.to(DEVICE)
    attention_mask = text_inputs.attention_mask.to(DEVICE)

    # === 4. æ‰§è¡Œæ¨ç† (ä¿®æ­£äº† ValueError å’Œ IndentationError) ===
    # ç¡®ä¿æ­¤è¡Œç›¸å¯¹äºå…¶ä¸Šä¸€ä¸ªé€»è¾‘è¡Œï¼ˆå¦‚ text_inputs = ...ï¼‰æ­£ç¡®ç¼©è¿›
    logits_per_image, _, _, _ = model(image_tensor, input_ids, attention_mask)

    # === 5. è¾“å‡ºç»“æœ ===
    # ç›¸ä¼¼åº¦ Logits è½¬ä¸ºæ¦‚ç‡
    probs = logits_per_image.softmax(dim=-1).squeeze(0)
    
    # è·å–æœ€é«˜æ¦‚ç‡çš„ç´¢å¼•
    best_match_index = probs.argmax().item()
    predicted_label = LABELS[best_match_index]
    
    # æ ¼å¼åŒ–è¾“å‡º
    print("-" * 60)
    print(f"æ¨ç†å›¾åƒè·¯å¾„: {IMAGE_PATH}")
    print(f"æœ€ç»ˆé¢„æµ‹ç»“æœ: ã€{predicted_label}ã€‘")
    print("-" * 60)
    print("æ ‡ç­¾ Softmax ç›¸ä¼¼åº¦å¾—åˆ†:")
    
    # æ‰“å°æ¯ä¸ªæ ‡ç­¾çš„æ¦‚ç‡ï¼Œå¹¶æŒ‰æ¦‚ç‡é™åºæ’åˆ—
    results = sorted(zip(LABELS, probs.tolist()), key=lambda x: x[1], reverse=True)
    
    for label, prob in results:
        print(f"  {label:<10}: {prob:.4f}")
    print("-" * 60)

if __name__ == "__main__":
    inference()