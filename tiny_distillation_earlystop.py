import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import argparse
from tqdm import tqdm
from transformers import CLIPTokenizer
from sklearn.model_selection import train_test_split
from torchvision import transforms
from torchvision.transforms.functional import InterpolationMode
import open_clip

# --------------------------- ä½ çš„å­¦ç”Ÿæ¨¡å‹ ---------------------------
# ç¡®ä¿ tiny_student_model.py å’Œå…¶ä¾èµ–ï¼ˆå¦‚ image_encoder.pyï¼‰åœ¨è·¯å¾„ä¸­
from tiny_student_model import LightweightStudentCLIP

# --------------------------- è·¯å¾„ ---------------------------
TEACHER_CHECKPOINT_PATH = "/root/mnist-clip/checkpoints/models--chendelong--RemoteCLIP/snapshots/bf1d8a3ccf2ddbf7c875705e46373bfe542bce38/RemoteCLIP-ViT-B-32.pt"
IMG_DIR = "/root/mnist-clip/data/RSICD_images"
JSON_PATH = "/root/mnist-clip/data/RSICD-en_cleaned.json"

# --------------------------- æ•°æ®é›†ï¼ˆè¿”å› PILï¼‰ ---------------------------
class RSICDDataset(Dataset):
    def __init__(self, data_list, img_dir):
        self.data_list = data_list
        self.img_dir = img_dir
    def __len__(self): return len(self.data_list)
    def __getitem__(self, idx):
        item = self.data_list[idx]
        # æ³¨æ„: ä½ çš„é”®åæ˜¯ "imged_id"ï¼Œä¿æŒä¸å˜
        img_path = os.path.join(self.img_dir, item["imged_id"]) 
        try:
            img = Image.open(img_path).convert('RGB')
        except:
            return None
        return img, item["caption"]

# --------------------------- æŸå¤±å‡½æ•° (æ¥å— alpha å’Œ temp å‚æ•°) ---------------------------
def compute_distillation_loss(s_logits, t_logits, alpha=0.1, temp=4.0, device="cuda"): # é»˜è®¤ alpha è®¾ä¸º 0.1
    # Hard Loss (å¯¹æ¯”å­¦ä¹ ä¸­çš„äº¤å‰ç†µ)
    # ç›®æ ‡æ˜¯ä¸»å¯¹è§’çº¿
    hard_loss = F.cross_entropy(s_logits, torch.arange(s_logits.size(0), device=device))
    
    # Soft Loss (KL æ•£åº¦)
    soft_targets = F.softmax(t_logits / temp, dim=-1)
    kd_loss = F.kl_div(F.log_softmax(s_logits / temp, dim=-1), soft_targets, reduction='batchmean') * (temp**2)
    
    # æ··åˆæŸå¤±
    return alpha * hard_loss + (1.0 - alpha) * kd_loss

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--batch_size', type=int, default=48)
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--lr', type=float, default=5e-5)
    parser.add_argument('--vision_variant', type=str, default='L1')
    parser.add_argument('--save_dir', type=str, default='./remoteclip_student_with_val2')
    parser.add_argument('--patience', type=int, default=7)
    
    # ğŸš€ å…³é”®ä¿®æ”¹ 1: æ·»åŠ è’¸é¦è¶…å‚æ•°
    parser.add_argument('--distill_T', type=float, default=4.0, help="è’¸é¦æ¸©åº¦ T")
    parser.add_argument('--distill_alpha', type=float, default=0.1, help="ç¡¬æ ‡ç­¾æŸå¤±æƒé‡ alpha (å»ºè®® 0.1~0.3)")
    
    args = parser.parse_args()
    os.makedirs(args.save_dir, exist_ok=True)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ==================== 1. åŠ è½½ Teacher ====================
    teacher_model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-32", pretrained=TEACHER_CHECKPOINT_PATH, device=device
    )
    teacher_model.eval()
    for p in teacher_model.parameters(): p.requires_grad = False
    teacher_tokenizer = open_clip.get_tokenizer('ViT-B-32')

    # ğŸš€ å…³é”®ä¿®æ”¹ 2: å®šä¹‰è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸“ç”¨é¢„å¤„ç†
    mean = [0.48145466, 0.4578275, 0.40821073]
    std = [0.26862954, 0.26130258, 0.27577711]

    # è®­ç»ƒé›†é¢„å¤„ç† (å¿…é¡»åŒ…å«éšæœºå¢å¼ºï¼Œè§£å†³è¿‡æ‹Ÿåˆ)
    train_preprocess = transforms.Compose([
        transforms.RandomResizedCrop(224, scale=(0.5, 1.0), interpolation=InterpolationMode.BICUBIC),
        transforms.RandomHorizontalFlip(p=0.5), # éšæœºç¿»è½¬
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std),
    ])

    # éªŒè¯é›†é¢„å¤„ç† (æ ‡å‡† CenterCropï¼Œç”¨äºç¨³å®šè¯„ä¼°)
    val_preprocess = transforms.Compose([
        transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std),
    ])
    # åŸå§‹çš„ teacher_preprocess å˜é‡å·²å¼ƒç”¨

    # ==================== 2. åŠ è½½ Student ====================
    student_model = LightweightStudentCLIP(vision_variant=args.vision_variant, projection_dim=512).to(device)
    student_model.train()
    for p in student_model.text_model.parameters(): p.requires_grad = False
    for p in student_model.text_projection.parameters(): p.requires_grad = True
    student_model.logit_scale.requires_grad = True
    student_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

    # ==================== 3. æ•°æ®åˆ’åˆ†ï¼š80% è®­ç»ƒ + 20% éªŒè¯ ====================
    with open(JSON_PATH) as f:
        data = json.load(f)
    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, stratify=None)
    print(f"è®­ç»ƒé›†: {len(train_data)} å¼ , éªŒè¯é›†: {len(val_data)} å¼ ")

    train_dataset = RSICDDataset(train_data, IMG_DIR)
    val_dataset   = RSICDDataset(val_data,   IMG_DIR)

    def collate_fn(batch):
        batch = [b for b in batch if b is not None]
        if len(batch) == 0: return None
        return [b[0] for b in batch], [b[1] for b in batch]

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,
                             num_workers=8, pin_memory=True, drop_last=True, collate_fn=collate_fn)
    val_loader   = DataLoader(val_dataset,   batch_size=args.batch_size, shuffle=False,
                             num_workers=8, pin_memory=True, drop_last=False, collate_fn=collate_fn)

    optimizer = torch.optim.AdamW(
        [p for p in student_model.parameters() if p.requires_grad],
        lr=args.lr, weight_decay=0.01, betas=(0.9, 0.98), eps=1e-6
    )

    # ==================== 4. è®­ç»ƒ + éªŒè¯ + æ—©åœ ====================
    best_val_loss = float('inf')
    patience_counter = 0
    best_epoch = 0

    for epoch in range(1, args.epochs + 1):
        # ----- Train -----
        student_model.train()
        train_loss = 0.0
        for pil_images, captions in tqdm(train_loader, desc=f"Train Epoch {epoch}"):
            if pil_images is None: continue

            with torch.no_grad():
                # ğŸš€ å…³é”®ä¿®æ”¹ 3: è®­ç»ƒæ—¶ä½¿ç”¨ train_preprocess (Teacher å’Œ Student çœ‹åˆ°å¢å¼ºå›¾)
                img_tensor = torch.stack([train_preprocess(img) for img in pil_images]).to(device)
                
                text_tokens = teacher_tokenizer(captions).to(device)
                img_f = teacher_model.encode_image(img_tensor)
                txt_f = teacher_model.encode_text(text_tokens)
                img_f = img_f / img_f.norm(dim=-1, keepdim=True)
                txt_f = txt_f / txt_f.norm(dim=-1, keepdim=True)
                logit_scale = teacher_model.logit_scale.exp()
                t_logits_i = logit_scale * img_f @ txt_f.t()
                t_logits_t = t_logits_i.t()

            text_inputs = student_tokenizer(captions, padding=True, truncation=True, max_length=77, return_tensors="pt").to(device)
            s_logits_i, s_logits_t = student_model(img_tensor, text_inputs.input_ids, text_inputs.attention_mask)

            # ğŸš€ å…³é”®ä¿®æ”¹ 4: è®­ç»ƒæŸå¤±ä½¿ç”¨ argparse ä¼ å…¥çš„å‚æ•°
            loss = (compute_distillation_loss(s_logits_i, t_logits_i, device=device, alpha=args.distill_alpha, temp=args.distill_T) +
                    compute_distillation_loss(s_logits_t, t_logits_t, device=device, alpha=args.distill_alpha, temp=args.distill_T)) / 2

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)

        # ----- Valid -----
        student_model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for pil_images, captions in val_loader:
                if pil_images is None: continue
                
                # ğŸš€ å…³é”®ä¿®æ”¹ 5: éªŒè¯æ—¶ä½¿ç”¨ val_preprocess (æ ‡å‡† CenterCrop)
                img_tensor = torch.stack([val_preprocess(img) for img in pil_images]).to(device)
                
                text_tokens = teacher_tokenizer(captions).to(device)
                img_f = teacher_model.encode_image(img_tensor)
                txt_f = teacher_model.encode_text(text_tokens)
                img_f = img_f / img_f.norm(dim=-1, keepdim=True)
                txt_f = txt_f / txt_f.norm(dim=-1, keepdim=True)
                logit_scale = teacher_model.logit_scale.exp()
                t_logits_i = logit_scale * img_f @ txt_f.t()
                t_logits_t = t_logits_i.t()

                text_inputs = student_tokenizer(captions, padding=True, truncation=True, max_length=77, return_tensors="pt").to(device)
                s_logits_i, s_logits_t = student_model(img_tensor, text_inputs.input_ids, text_inputs.attention_mask)

                # éªŒè¯æŸå¤±ä½¿ç”¨ argparse ä¼ å…¥çš„å‚æ•°
                loss = (compute_distillation_loss(s_logits_i, t_logits_i, device=device, alpha=args.distill_alpha, temp=args.distill_T) +
                        compute_distillation_loss(s_logits_t, t_logits_t, device=device, alpha=args.distill_alpha, temp=args.distill_T)) / 2
                val_loss += loss.item()
        val_loss /= len(val_loader)

        print(f"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}", end=" ")

        # ----- æ—©åœ + ä¿å­˜æœ€ä½³æ¨¡å‹ -----
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch
            patience_counter = 0
            torch.save(student_model.state_dict(), f"{args.save_dir}/BEST_student_model.pt")
            print("â† New Best!")
        else:
            patience_counter += 1
            print(f"(patience {patience_counter}/{args.patience})")

        if patience_counter >= args.patience:
            print(f"\næ—©åœè§¦å‘ï¼æœ€ä½³æ¨¡å‹åœ¨ Epoch {best_epoch}ï¼ŒVal Loss = {best_val_loss:.4f}")
            break

        # æ¯ 5 è½®ä¹Ÿä¿å­˜ä¸€ä¸‹
        if epoch % 5 == 0:
            torch.save(student_model.state_dict(), f"{args.save_dir}/student_epoch{epoch}.pt")

    print(f"è®­ç»ƒç»“æŸï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³ {args.save_dir}/BEST_student_model.pt")

if __name__ == "__main__":
    main()