import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
import json
import time
import sys
from torch.optim.lr_scheduler import CosineAnnealingLR 
from transformers import CLIPModel, CLIPTokenizer
from sklearn.model_selection import train_test_split

# ä¾èµ–æ‚¨çš„æœ¬åœ°æ–‡ä»¶
try:
    # ---!!! ä¿®æ”¹ç‚¹ 1: å¯¼å…¥è½»é‡åŒ–å­¦ç”Ÿæ¨¡å‹ !!!---
    from tiny_student_model import LightweightStudentCLIP as StudentCLIP 
    # å‡è®¾ image_encoder å­˜åœ¨äº StudentCLIP çš„å¯¼å…¥è·¯å¾„ä¸­
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿ tiny_student_model.py (åŒ…å« LightweightStudentCLIP) å’Œ image_encoder.py åœ¨å½“å‰ç›®å½•ä¸‹ã€‚")
    sys.exit(1)


# --- 1. é…ç½®å‚æ•° ---
# ---!!! ä¿®æ”¹ç‚¹ 2: æ›´æ–°æ•°æ®è·¯å¾„ !!!---
# æ•°æ®é›†å›¾ç‰‡è·¯å¾„ (æ ¹æ®æ‚¨çš„æè¿°)
DATA_DIR = "/root/mnist-clip/data/RSICD_images"
# æè¿°æ–‡ä»¶è·¯å¾„ (æ ¹æ®æ‚¨çš„æè¿°)
CAPTION_FILE_NAME = "RSICD-en_cleaned.json" 
# CAPTION_FILE_PATH åº”è¯¥æŒ‡å‘ JSON æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•
CAPTION_FILE_PATH = os.path.join("/root/mnist-clip/data", CAPTION_FILE_NAME) 

# Teacher Model è·¯å¾„ (ä¿æŒä¸å˜)
REMOTECLIP_PATH = "checkpoints/models--chendelong--RemoteCLIP/snapshots/bf1d8a3ccf2ddbf7c875705e46373bfe542bce38/RemoteCLIP-ViT-B-32.pt"

# è®­ç»ƒå‚æ•° (å¯ä»¥æ ¹æ®æ‚¨çš„ç¡¬ä»¶è°ƒæ•´)
BATCH_SIZE = 64  
NUM_EPOCHS = 15      
LEARNING_RATE = 5e-5 
TRAIN_SPLIT_RATIO = 0.8
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ä½¿ç”¨ Teacher çš„åŸºå‡†æ¨¡å‹ä½œä¸º Tokenizer/ç»“æ„å‚è€ƒ
TEACHER_MODEL_NAME = 'openai/clip-vit-base-patch32' 
MAX_TEXT_LENGTH = 77 

# --- çŸ¥è¯†è’¸é¦è¶…å‚æ•° ---
TEMPERATURE = 4.0   
ALPHA = 0.5         
PATIENCE = 5        


# --- 2. æ•™å¸ˆæ¨¡å‹åŠ è½½å‡½æ•° (ä¿æŒä¸å˜) ---
def load_remoteclip_teacher(model_path, device):
    """åŠ è½½ RemoteCLIP é¢„è®­ç»ƒæƒé‡åˆ°æ ‡å‡† CLIP æ¨¡å‹ç»“æ„ä¸­"""
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½ Teacher æ¨¡å‹: {model_path}...")
    
    try:
        state_dict = torch.load(model_path, map_location=device)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•™å¸ˆæ¨¡å‹æƒé‡æ–‡ä»¶: {model_path}")
        sys.exit(1)
    
    # å®ä¾‹åŒ–æ ‡å‡† CLIP æ¨¡å‹ (Teacher æ˜¯ ViT-B/32)
    teacher_model = CLIPModel.from_pretrained(TEACHER_MODEL_NAME).to(device)

    try:
        teacher_model.load_state_dict(state_dict, strict=True) 
    except RuntimeError as e:
        print("âš ï¸ æ— æ³•ç›´æ¥åŠ è½½ RemoteCLIP æƒé‡ã€‚è¯·ç¡®ä¿æƒé‡æ–‡ä»¶ç»“æ„ä¸ CLIPModel åŒ¹é…ã€‚")
        print(f"åŸå§‹åŠ è½½é”™è¯¯ä¿¡æ¯: {e}")
        
    teacher_model.eval()
    # å†»ç»“æ‰€æœ‰ Teacher å‚æ•°
    for param in teacher_model.parameters():
        param.requires_grad = False
        
    return teacher_model


# --- 3. è‡ªå®šä¹‰æ•°æ®é›†ç±» (!!! éœ€è¦è°ƒæ•´ï¼Œå› ä¸ºæ‚¨çš„ json æ–‡ä»¶æ ¼å¼ä¸åŒ!!!) ---
class RemoteSensingDataset(Dataset):
    def __init__(self, data_list, image_dir, tokenizer, transform):
        self.data_list = data_list
        self.image_dir = image_dir
        self.tokenizer = tokenizer
        self.transform = transform

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        item = self.data_list[idx]
        
        # ---!!! ä¿®æ”¹ç‚¹ 3: é€‚åº”æ–°çš„ JSON æ–‡ä»¶é”®å ('imged_id' å’Œ 'caption') !!!---
        image_filename = item['imged_id']
        caption = item['caption']
        # ------------------------------------------------------------------------
        
        img_path = os.path.join(self.image_dir, image_filename)
        
        try:
            image = Image.open(img_path).convert("RGB")
            image = self.transform(image)
        except Exception as e:
            # print(f"âš ï¸ æ— æ³•åŠ è½½å›¾ç‰‡ {img_path}: {e}") # è°ƒè¯•æ—¶å¯ç”¨
            return None 

        tokenized_text = self.tokenizer(
            caption, 
            padding='max_length', 
            truncation=True, 
            max_length=MAX_TEXT_LENGTH, 
            return_tensors="pt"
        )
        
        return image, tokenized_text['input_ids'].squeeze(), tokenized_text['attention_mask'].squeeze()


# --- 4. æ•°æ®åŠ è½½ (ä¿æŒå¢å¼ºé€»è¾‘ï¼Œæ›´æ–°è·¯å¾„) ---
def load_and_split_data():
    if not os.path.exists(CAPTION_FILE_PATH):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æè¿°æ–‡ä»¶ï¼Œè¯·ç¡®ä¿æ–‡ä»¶ä½äº: {CAPTION_FILE_PATH}")
        sys.exit(1)
        
    with open(CAPTION_FILE_PATH, 'r', encoding='utf-8') as f:
        full_data = json.load(f)

    print(f"âœ… æˆåŠŸåŠ è½½ {len(full_data)} æ¡æ•°æ®ã€‚")

    train_data, val_data = train_test_split(
        full_data, 
        test_size=(1 - TRAIN_SPLIT_RATIO), 
        random_state=42 
    )
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_data)} | éªŒè¯é›†å¤§å°: {len(val_data)}")
    
    # å›¾åƒé¢„å¤„ç†ä¿æŒä¸å˜ (é€‚é… CLIP/RemoteCLIP ViT-B/32 çš„è¾“å…¥)
    train_preprocess = transforms.Compose([
        transforms.RandomHorizontalFlip(), 
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05), 
        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], 
                             std=[0.26862954, 0.26130258, 0.27577711])
    ])
    
    val_preprocess = transforms.Compose([
        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], 
                             std=[0.26862954, 0.26130258, 0.27577711])
    ])
    
    tokenizer = CLIPTokenizer.from_pretrained(TEACHER_MODEL_NAME)

    train_dataset = RemoteSensingDataset(train_data, DATA_DIR, tokenizer, train_preprocess)
    val_dataset = RemoteSensingDataset(val_data, DATA_DIR, tokenizer, val_preprocess)
    
    def custom_collate_fn(batch):
        batch = [item for item in batch if item is not None]
        if not batch:
            return None, None, None
        
        images, input_ids, attention_masks = zip(*batch)
        
        return (
            torch.stack(images),
            torch.stack(input_ids),
            torch.stack(attention_masks)
        )

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, num_workers=4) 
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn, num_workers=4)
    
    return train_loader, val_loader


# --- 5. æŸå¤±å‡½æ•° (ä¿æŒä¸å˜) ---

def contrastive_loss_hard(logits):
    """æ ‡å‡†çš„ CLIP å¯¹æ¯”å­¦ä¹ æŸå¤± (äº¤å‰ç†µ)"""
    targets = torch.arange(logits.shape[0]).long().to(DEVICE)
    return nn.CrossEntropyLoss()(logits, targets)

def compute_distillation_loss(student_logits, teacher_logits, alpha, temperature, device):
    """è®¡ç®—ç»„åˆæŸå¤±ï¼šALPHA * ç¡¬æŸå¤± + (1 - ALPHA) * è½¯è’¸é¦æŸå¤±"""
    
    # 1. è½¯ç›®æ ‡è’¸é¦æŸå¤± (Soft KD Loss)
    soft_targets = F.softmax(teacher_logits / temperature, dim=-1)
    
    kd_loss = nn.KLDivLoss(reduction='batchmean')(
        F.log_softmax(student_logits / temperature, dim=-1),
        soft_targets
    ) * (temperature ** 2) 

    # 2. ç¡¬ç›®æ ‡å¯¹æ¯”æŸå¤± (Hard Contrastive Loss)
    hard_loss = contrastive_loss_hard(student_logits)

    # 3. ç»„åˆæŸå¤±
    combined_loss = alpha * hard_loss + (1.0 - alpha) * kd_loss
    return combined_loss


# --- 6. è®­ç»ƒå’ŒéªŒè¯å‡½æ•° (ä¿æŒä¸å˜) ---

def train_one_epoch(student_model, teacher_model, dataloader, optimizer, scheduler, epoch):
    student_model.train()
    total_loss = 0.0
    start_time = time.time()
    
    for step, batch in enumerate(dataloader):
        images, input_ids, attention_mask = batch
        
        if images is None:
            continue
            
        images = images.to(DEVICE)
        input_ids = input_ids.to(DEVICE)
        attention_mask = attention_mask.to(DEVICE)
        
        optimizer.zero_grad()
        
        # --- Teacher Model å‰å‘ä¼ æ’­ (æ— æ¢¯åº¦) ---
        with torch.no_grad():
            teacher_outputs = teacher_model(
                pixel_values=images, 
                input_ids=input_ids, 
                attention_mask=attention_mask,
                return_loss=False 
            )
            
            t_img_embeds = teacher_outputs.image_embeds / teacher_outputs.image_embeds.norm(dim=-1, keepdim=True)
            t_text_embeds = teacher_outputs.text_embeds / teacher_outputs.text_embeds.norm(dim=-1, keepdim=True)
            
            teacher_logit_scale = teacher_model.logit_scale.exp()
            teacher_logits_per_image = teacher_logit_scale * t_img_embeds @ t_text_embeds.T
            teacher_logits_per_text = teacher_logits_per_image.T
        
        # --- Student Model å‰å‘ä¼ æ’­ ---
        # LightweightStudentCLIP çš„ forward æ–¹æ³•è¿”å› (logits_per_image, logits_per_text)
        logits_per_image, logits_per_text = student_model(
            images, 
            input_ids, 
            attention_mask
        )
        
        # è®¡ç®—ç»„åˆè’¸é¦æŸå¤±
        loss_i = compute_distillation_loss(logits_per_image, teacher_logits_per_image, ALPHA, TEMPERATURE, DEVICE)
        loss_t = compute_distillation_loss(logits_per_text, teacher_logits_per_text, ALPHA, TEMPERATURE, DEVICE)
        
        loss = (loss_i + loss_t) / 2
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        if (step + 1) % 100 == 0:
            avg_loss = total_loss / (step + 1)
            current_lr = optimizer.param_groups[0]['lr']
            elapsed = time.time() - start_time
            print(f"Epoch {epoch}/{NUM_EPOCHS} | Step {step+1} | Distill Loss: {avg_loss:.4f} | LR: {current_lr:.2e} | Time: {elapsed:.2f}s")
            
    scheduler.step()
    
    return total_loss / len(dataloader)


@torch.no_grad()
def validate(model, dataloader):
    model.eval()
    total_val_loss = 0.0
    
    for batch in dataloader:
        images, input_ids, attention_mask = batch
        
        if images is None:
            continue
            
        images = images.to(DEVICE)
        input_ids = input_ids.to(DEVICE)
        attention_mask = attention_mask.to(DEVICE)
        
        # LightweightStudentCLIP çš„ forward æ–¹æ³•è¿”å› (logits_per_image, logits_per_text)
        logits_per_image, logits_per_text = model(
            images, 
            input_ids, 
            attention_mask
        )
        
        # åªä½¿ç”¨æ ‡å‡†çš„ç¡¬å¯¹æ¯”æŸå¤±è¿›è¡ŒéªŒè¯
        loss_i = contrastive_loss_hard(logits_per_image)
        loss_t = contrastive_loss_hard(logits_per_text)
        loss = (loss_i + loss_t) / 2
        
        total_val_loss += loss.item()

    if len(dataloader) > 0:
        return total_val_loss / len(dataloader)
    return 0.0


# --- 7. ä¸»ç¨‹åº (æ›´æ–°å­¦ç”Ÿæ¨¡å‹åˆå§‹åŒ–å’Œå†»ç»“é€»è¾‘) ---
def main():
    print(f"ğŸš€ å¼€å§‹ StudentCLIP çŸ¥è¯†è’¸é¦è®­ç»ƒ (è®¾å¤‡: {DEVICE})")

    # æ­¥éª¤ 1: åŠ è½½æ•°æ®å¹¶åˆ’åˆ†
    train_loader, val_loader = load_and_split_data()

    # æ­¥éª¤ 2: åˆå§‹åŒ– Teacher å’Œ Student æ¨¡å‹
    teacher_model = load_remoteclip_teacher(REMOTECLIP_PATH, DEVICE)
    
    # ---!!! ä¿®æ”¹ç‚¹ 4: åˆå§‹åŒ– LightweightStudentCLIP !!!---
    # LightweightStudentCLIP çš„é»˜è®¤å‚æ•°ä¸ student_large_distill_train.py çš„é€»è¾‘åŒ¹é…
    student_model = StudentCLIP().to(DEVICE) 
    # --------------------------------------------------------
    
    # å†»ç»“ Student æ–‡æœ¬ç¼–ç å™¨å‚æ•° (ä¸ large è„šæœ¬çš„å†»ç»“é€»è¾‘ä¿æŒä¸€è‡´ï¼Œåªè®­ç»ƒè§†è§‰å’ŒæŠ•å½±å±‚)
    print("ğŸ”’ å†»ç»“ Student Model æ–‡æœ¬ç¼–ç å™¨å‚æ•°...")
    for param in student_model.text_model.parameters():
        param.requires_grad = False
        
    # ç¡®ä¿ logit_scale å’ŒæŠ•å½±å±‚å¯è®­ç»ƒ
    student_model.logit_scale.requires_grad = True
    for param in student_model.text_projection.parameters():
        param.requires_grad = True
    
    # ---!!! ä¿®æ”¹ç‚¹ 5: ä¼˜åŒ–å™¨åªå…³æ³¨å¯è®­ç»ƒå‚æ•° !!!---
    # åªéœ€è¦è®­ç»ƒè§†è§‰ç¼–ç å™¨ã€logit_scale å’Œ æ–‡æœ¬æŠ•å½±å±‚ã€‚
    trainable_params = filter(lambda p: p.requires_grad, student_model.parameters())
    optimizer = torch.optim.AdamW(trainable_params, lr=LEARNING_RATE)
    
    # å¼•å…¥å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)

    best_val_loss = float('inf')
    epochs_no_improve = 0 
    
    # æ­¥éª¤ 3: è®­ç»ƒå¾ªç¯
    for epoch in range(1, NUM_EPOCHS + 1):
        train_loss = train_one_epoch(student_model, teacher_model, train_loader, optimizer, scheduler, epoch)
        
        val_loss = validate(student_model, val_loader)
        
        print(f"\n======== Epoch {epoch} Summary ========")
        print(f"Average Training Distillation Loss: {train_loss:.4f}")
        print(f"Average Validation Hard Loss: {val_loss:.4f}")
        print("=======================================\n")
        
        # --- æ—©åœå’Œä¿å­˜æœ€ä½³æ¨¡å‹é€»è¾‘ ---
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            epochs_no_improve = 0 
            # ä¿å­˜çš„æ˜¯ StudentCLIP çš„çŠ¶æ€å­—å…¸
            model_save_path = f"./student_clip_remote_distilled_best_model.pth" 
            torch.save(student_model.state_dict(), model_save_path)
            print(f"âœ¨ éªŒè¯æŸå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ {model_save_path}")
        else:
            epochs_no_improve += 1
            print(f"âš ï¸ éªŒè¯æŸå¤±æœªé™ä½. Patience: {epochs_no_improve}/{PATIENCE}")

        if epochs_no_improve == PATIENCE:
            print(f"ğŸ›‘ æå‰åœæ­¢è®­ç»ƒ! éªŒè¯æŸå¤±å·²è¿ç»­ {PATIENCE} ä¸ª Epoch æœªé™ä½ã€‚")
            break 

if __name__ == "__main__":
    main()